{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot using NLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-lkTzgwc5yO"
      },
      "outputs": [],
      "source": [
        "!pip install nltk\n",
        "!pip install --upgrade spacy\n",
        "!pip install --upgrade spacy[cuda111,transformers]\n",
        "\n",
        "!pip install newspaper3k\n",
        "!python -m spacy download en_core_web_lg\n",
        "!python -m spacy download en_core_web_trf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
        "from tqdm.autonotebook import tqdm\n",
        "from spacy import displacy\n",
        "from newspaper import Article\n",
        "import re\n",
        "import spacy\n",
        "import nltk\n",
        "import random\n",
        "import string\n",
        "import numpy as np\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# scraping and downloading the data from the website\n",
        "\n",
        "article=Article('https://www.gov.uk/student-visa')\n",
        "article.download()\n",
        "article.parse()\n",
        "article.nlp()\n",
        "\n",
        "corpus = article.text\n",
        "\n",
        "# Text Pre-processing\n",
        "\n",
        "def clean_text(corpus):\n",
        "  newline_re = r'[\\n]+'\n",
        "  white_space_re = r\"\\s+\"\n",
        "\n",
        "  corpus = re.sub(newline_re, r' ', corpus, flags=re.UNICODE)\n",
        "  corpus = re.sub(white_space_re, r' ', corpus, flags=re.UNICODE)\n",
        "\n",
        "  return corpus\n",
        "\n",
        "def all_steps(text, steps):\n",
        "  steps_dict = {'clean': clean_text}\n",
        "\n",
        "  if len(steps):\n",
        "    for step in steps:\n",
        "      text = steps_dict[step](text)\n",
        "  return text\n",
        "\n",
        "text = corpus\n",
        "steps = ['clean']\n",
        "data = all_steps(text, steps)\n",
        "\n",
        "text = data\n",
        "\n",
        "sentence_list = nltk.sent_tokenize(text)\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "#passing the data in the pipeline\n",
        "\n",
        "just_text = nltk.word_tokenize(text)\n",
        "docs = list(tqdm(nlp.pipe(just_text), total=len(just_text)))\n",
        "\n",
        "[(i.text, i.ent_iob_ + \"-\" + i.ent_type_) for i in doc[0:30]]\n",
        "\n",
        "doc = nlp(text)\n",
        "sent = list(doc.sents)[1]\n",
        "\n",
        "nlp_trf = spacy.load(\"en_core_web_trf\")\n",
        "\n",
        "doc = nlp_trf(text)\n",
        "\n",
        "# Chatbot functionality implementation\n",
        "\n",
        "model_name = \"deepset/roberta-base-squad2\"\n",
        "\n",
        "hugg = pipeline('question-answering', model=model_name, tokenizer=model_name)"
      ],
      "metadata": {
        "id": "fI12eUPPdDBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Question and Answering\n",
        "\n",
        "QA_input = {\n",
        "    'question': \"what is skilled worker visa?\",\n",
        "    'context': text\n",
        "}\n",
        "res = hugg(QA_input)\n",
        "\n",
        "print(res)"
      ],
      "metadata": {
        "id": "8yaEXXs0demc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}